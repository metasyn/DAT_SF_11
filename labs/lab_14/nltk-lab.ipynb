{
 "metadata": {
  "name": "",
  "signature": "sha256:535c2dcfa38bfa2d3a5d48dbccd92d8595a5bdb820a14f870d28a3eff7f0513d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If this gives error, go to command line and run \"conda install nltk\"\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NLTK comes with its own set of corpus data that we can use to learn about NLTK\n",
      "# nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Corpus\n",
      "\n",
      "* \"corpus\" = collection of documents\n",
      "* \"corpora\" = plural form of corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import inaugural"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inaugural.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Reading Wine Reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import webtext\n",
      "webtext.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# wine reviews corpus\n",
      "text = webtext.raw('wine.txt')\n",
      "print text[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tokenization\n",
      "\n",
      "What:  Separate text into units such as sentences or words\n",
      "\n",
      "Why:   Gives structure to previously unstructured text\n",
      "\n",
      "Notes: \n",
      "* Relatively easy with English language text, not easy with some languages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Explore the different tokenizers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize into sentences\n",
      "sentences = [sent for sent in tokenize.sent_tokenize(text)]\n",
      "sentences[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize into words\n",
      "tokens = [word for word in tokenize.word_tokenize(text)]\n",
      "tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# only keep tokens that start with a letter (using regular expressions)\n",
      "import re\n",
      "clean_tokens = [token for token in tokens if re.search(r'^[a-zA-Z]+', token)]\n",
      "clean_tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look at word frequency\n",
      "from collections import Counter\n",
      "\n",
      "c = Counter(clean_tokens)\n",
      "c.most_common(25)       \n",
      "\n",
      "# You will see a mixed case words, stop words?, singular, plurals?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stemming\n",
      "\n",
      "What:  Reduce a word to its base/stem/root form\n",
      "\n",
      "Why:   Often makes sense to treat multiple word forms the same way\n",
      "\n",
      "Notes: \n",
      "* Uses a \"simple\" and fast rule-based approach\n",
      "* Output can be undesirable for irregular words\n",
      "* Stemmed words are usually not shown to users (used for analysis/indexing)\n",
      "* Some search engines treat words with the same stem as synonyms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import stem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = stem.SnowballStemmer('english')\n",
      "\n",
      "# example stemming\n",
      "print stemmer.stem('charge')\n",
      "print stemmer.stem('charging')\n",
      "print stemmer.stem('charged')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stem the tokens\n",
      "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
      "stemmed_tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Lemmatization\n",
      "\n",
      "What:  Derive the canonical form ('lemma') of a word\n",
      "\n",
      "Why:   Can be better than stemming\n",
      "\n",
      "Notes: Uses a dictionary-based approach (slower than stemming)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer = stem.WordNetLemmatizer()\n",
      "\n",
      "# example stemming\n",
      "print lemmatizer.lemmatize('charge')\n",
      "print lemmatizer.lemmatize('charging')\n",
      "print lemmatizer.lemmatize('charged')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stem the tokens\n",
      "stemmed_tokens = [lemmatizer.lemmatize(t) for t in clean_tokens]\n",
      "stemmed_tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stopword Removal\n",
      "\n",
      "What:  Remove common words that will likely appear in any text\n",
      "\n",
      "Why:   They don't tell you much about your text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# view the list of stopwords\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "sorted(stopwords)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stem the stopwords\n",
      "stemmed_stops = [stemmer.stem(t) for t in stopwords]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove stopwords from stemmed tokens\n",
      "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if stemmer.stem(t) not in stopwords]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look at word frequency\n",
      "from collections import Counter\n",
      "\n",
      "c = Counter(stemmed_tokens_no_stop)\n",
      "c.most_common(25) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def zipf_plot(text):\n",
      "    '''Creates a log-log plot of word frequency vs word rank.\n",
      "    \"a function to process a large text and plot word frequency against word rank using pylab.plot\"\n",
      "\n",
      "    Precondition: 'text' contains a list of words.\n",
      "    '''\n",
      "    fdist = nltk.FreqDist(text)\n",
      "    \n",
      "    # figured out pylab.plot syntax, sadly, from random googling\n",
      "    plt.plot(\n",
      "            range(1, fdist.B() + 1),      # x-axis: word rank\n",
      "            fdist.values()                # y-axis: word count\n",
      "            )   \n",
      "    plt.xscale('log')\n",
      "    plt.yscale('log')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.FreqDist(stemmed_tokens_no_stop)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zipf_plot(stemmed_tokens_no_stop)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classify Names as Male or Female"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import names\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names.words('male.txt')[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names.words('female.txt')[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names_data = [(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(names_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names_data[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features(word):\n",
      "    features = {}\n",
      "    features['last_letter'] = word[-1]\n",
      "#     features['first_letter'] = word[0]\n",
      "#     features['length'] = len(word)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gender_features('Ricky')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import apply_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = apply_features(gender_features, names_data[500:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = apply_features(gender_features, names_data[:500])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Score the Accuracy of the names classifier we built\n",
      "nltk.classify.accuracy(clf, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.show_most_informative_features(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Movie Reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's classify Movie Reviews\n",
      "df = pd.read_csv('../data/movie-reviews-dataset.tsv', sep='\\t', names=['rating', 'review'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.values[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def stem_review(category, text):\n",
      "    # Let's use Porter Stemmer, but you can use Snowball Stemmer or Lammatizer\n",
      "    stemmer = stem.PorterStemmer()\n",
      "    stopwords = nltk.corpus.stopwords.words('english')\n",
      "\n",
      "    # Tokenize the words\n",
      "    words = tokenize.word_tokenize(text)\n",
      "    words = [token for token in words if re.search(r'^[a-zA-Z]+', token)]\n",
      "    \n",
      "    # Stem the words\n",
      "    stem_words = [stemmer.stem(word) for word in words if word not in stopwords]\n",
      "    return (stem_words, category)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents = []\n",
      "for cat, review in df.values:\n",
      "    review = unicode(review, 'utf8')\n",
      "    review.encode('utf8','ignore')\n",
      "    documents.append(stem_review(cat, review))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents[0:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Number of Reviews\n",
      "len(documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to convert document words into \"Feature\" Dictionary required by NLTK Classifier\n",
      "def document_features(document_words): # [_document-classify-extractor]\n",
      "    features = {}\n",
      "    for word in document_words:\n",
      "        features[word] = True\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "document_features([u'simplistic', u'tedious'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(document_features(d), c) for (d,c) in documents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shuffle the data before doing the training\n",
      "random.shuffle(featuresets)\n",
      "\n",
      "train_set, test_set = featuresets[:8000], featuresets[8000:]\n",
      "\n",
      "# Build the NaiveBayes Classifier\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nltk.classify.accuracy(classifier, test_set) \n",
      "classifier.show_most_informative_features(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
      "\n",
      "What:  Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
      "\n",
      "Why:   More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
      "\n",
      "Notes: Used for search engine scoring, text summarization, document clustering\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories = []\n",
      "reviews = []\n",
      "for words, cat in documents:\n",
      "    reviews.append(' '.join(words))\n",
      "    categories.append(cat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "tfidf = TfidfVectorizer()\n",
      "X = tfidf.fit_transform(reviews).toarray()\n",
      "lbl = LabelEncoder()\n",
      "y = lbl.fit_transform(categories)\n",
      "tfidf.get_feature_names()[-10:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "\n",
      "sk_clf = GaussianNB()\n",
      "sk_clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sk_clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your Turn - Build a Classifier to identify Spam from Ham\n",
      "\n",
      "* Use Dataset in labs/data/SpamCollection.tsv"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's classify emails\n",
      "df = pd.read_csv('../data/SpamCollection.tsv', sep='\\t', names=['mail_type', 'text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}