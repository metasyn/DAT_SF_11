{
 "metadata": {
  "name": "",
  "signature": "sha256:5ad8888ee2cf9ba4bf9dc88d6cb7d552e67a038e2fc8d74f930a47c1b70c3b4f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Lab - Principal Component Analysis\n",
      "In this lab we will investigate PCA applications.\n",
      "\n",
      "Credit: this notebook is updated and adapted from a tutorial by [Andreas Mueller](https://twitter.com/t3kcit)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.core.pylabtools import figsize\n",
      "%matplotlib inline\n",
      "figsize(12,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below will generate a random normally distributed matrix with the given covariance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Cov = np.array([[2, -2], [-2, 3]])\n",
      "X = np.random.multivariate_normal([1,2], Cov, size=200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(4, suppress=True) # show only four decimals\n",
      "print X[:10,:] # print the first 10 rows of X (from 0 to 9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = plt.scatter(X[:,0], X[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good linear fit?\n",
      "\n",
      "Let's look at the actual covariance matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.cov(X,rowvar=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PCA\n",
      "\n",
      "`sklearn.decomposition.PCA` implements `fit` and `transform` methods which do the following:\n",
      "\n",
      "1. place the point cloud in the center (0,0) and\n",
      "2. rotate it, such that the direction with most variance is parallel to the x-axis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_pca = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.mean_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look the resulting, transformed data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = plt.scatter(X_pca[:,0], X_pca[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Q: What is the covariances between these two axes?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.cov(X_pca, rowvar=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Technique: High Dimensional Analysis\n",
      "PCA can be used to recover signals in noise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1)\n",
      "X_HD = np.dot(X,np.random.uniform(0.2,3,(2,4))*(np.random.randint(0,2,(2,4))*2-1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hd_df = pd.DataFrame(X_HD)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hd_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have actually investigated the idea of PCA visually, using scatter matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = pd.scatter_matrix(hd_df,diagonal='kde')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stare at this long enough and you will realize this 4-dimensional data set is actually only 2-dimensions under the hood.\n",
      "\n",
      "...Or, we could just use PCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_HE = pca.fit_transform(X_HD)\n",
      "print X_HE[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have projected all 4 dimensions onto only 2 dimensions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X_HE[:,0], X_HE[:,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "he_df = pd.DataFrame(X_HE)\n",
      "_ = pd.scatter_matrix(he_df,diagonal='kde')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Technique - Dimensionality Reduction\n",
      "The underlying signal in our 4-dimensional data set is 2-dimensional (2 signal features and 2 noise features), so we can toss out 2 dimensions and exactly recover the signal, but this is not usually the case in real data (where all features have some signal AND some noise).\n",
      "\n",
      "SO, can we reduce the dimensionality even more and still reconstruct our signal?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_HD"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(1)\n",
      "X_E = pca.fit_transform(X_HD)\n",
      "print X_E[:10,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reconstructing data from PCA requires applying the inverse transform (luckily, there's a method for that):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_reconstructed = pca.inverse_transform(X_E)\n",
      "for i in xrange(4):\n",
      "    for j in xrange(4):\n",
      "        plt.subplot(4, 4, i * 4 + j + 1)\n",
      "        plt.scatter(X_HD[:,i], X_HD[:,j],c='r')\n",
      "        plt.scatter(X_reconstructed[:,i], X_reconstructed[:,j],c='b')\n",
      "        plt.axis('equal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Application - Image analysis\n",
      "This is PCA on truly high dimensional data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_mldata\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.utils import shuffle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_digits, _,_, y_digits = fetch_mldata(\"MNIST Original\").values() \n",
      "X_digits, y_digits = shuffle(X_digits,y_digits) \n",
      "X_digits = X_digits[-5000:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's have a look at some of the instances in the dataset we just loaded:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_mnist_image_matrix(data,size):\n",
      "    plt.rc(\"image\", cmap=\"binary\")\n",
      "    for i in xrange(size[0]*size[1]):\n",
      "        plt.subplot(size[0],size[1],i+1)\n",
      "        plt.imshow(data[i].reshape(28,28))\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())\n",
      "    plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_mnist_image_matrix(X_digits,[3,8])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA()\n",
      "X2_digits = pca.fit_transform(X_digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W = pca.components_\n",
      "plot_mnist_image_matrix(W,[1,8])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Umm....huh?\n",
      "\n",
      "Think of this as visualizing the ranked *best mean* images. So components[0:10] are pretty clear..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_mnist_image_matrix(W,[1,10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But components[100:] are mostly noise"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_mnist_image_matrix(W[100:],[1,10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W = pca.components_\n",
      "\n",
      "plot_mnist_image_matrix(W,[2,10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We quantify this observation with the `explained_variance_` variable"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(pca.explained_variance_ratio_);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We consider 784 components and we could comfortably reduce this to 100 with very little loss."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Don't believe me...I'll prove it!\n",
      "We will reconstruct the data set from this reduced projections. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(20)\n",
      "X2_few_digits = pca.fit_transform(X_digits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(16,4))\n",
      "X_recons_digits = pca.inverse_transform(X2_few_digits)\n",
      "plot_mnist_image_matrix(X_recons_digits,[2,8])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_mnist_image_matrix(X_digits,[2,2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_mnist_image_matrix(X_recons_digits,[2,8])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "EXIT_SUCCESS;"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- What happens, when you multiply one of the data axis with a large (or small) number? \n",
      "\n",
      "  Does the result stay the same? Why/why not?\n",
      "\n",
      "-----\n",
      "\n",
      "- Try to explore the iris dataset below using PCA. \n",
      "\n",
      "  What happens if you visualize the *last* two components of PCA instead of the first two?\n",
      "\n",
      "-----\n",
      "\n",
      "- Use PCA with 1, 2 or 3 axes and reconstruct the Iris data from each. How do the results change?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "_,data,target,_,_ = datasets.load_iris().values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}